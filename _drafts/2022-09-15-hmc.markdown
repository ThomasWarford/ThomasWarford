---
layout: post
title:  "Multi-GPU Physics Based Hamiltonian Monte
Carlo"
date:   2022-09-15
categories:
---

<div class="editors-note">
    <strong>Editor's Note:</strong> This post was originally written with Bruno Carillo Rodriquez during PRACE Summer of HPC. I've put it here because the website for SoHPC is no longer up.
</div>

## HMC in Probabilistic Programming

HMC is widely used in probabilistic programming, especially for fitting many-parameter models. Our implementation is scalable, portable, and based on Physics.

![HMC Animation]({{site.baseurl}}/assets/images/hmc/HMC.gif)

### Introduction

Generating samples from a given probability distribution is a surprisingly difficult task. Hamiltonian Monte Carlo (HMC) solves this challenge elegantly. HMC is widely used in probabilistic programming libraries, such as [Stan](https://mc-stan.org/docs/reference-manual/index.html) and [Numpyro](https://num.pyro.ai/en/stable/mcmc.html), for sampling parameters from posterior distributions in large statistical models like those used in epidemiology.

We had two main goals for this project:
1. Implement HMC from scratch in Python, based on a physics-driven approach. We represented the system as particles in a multidimensional space, each with its own mass and temperature as parameters.
2. Adapt the implementation for multi-CPU or GPU clusters by changing just one line of code.

| ![Metropolis-Hastings Algorithm]({{site.baseurl}}/assets/images/hmc/metropolis-hastings.gif) |
|:--:| 
| *Proposals generated by the Metropolis-Hastings algorithm with guassian step proposals. Note the high rejection rate and correlation, even with two dimensions.* |

### Theory

In Bayesian inference, given a model and some data, the probability distribution of the model parameters can be deduced. Generating samples from this distribution can be done via HMC.

#### Bayes Rule

Bayes' theorem states:

\[
p(\mathbf{q}|y)=\frac{p(\mathbf{q}) p(y|\mathbf{q})}{p(y)},
\]

where \( p(\mathbf{q}|y) \) is the posterior probability of the model parameters given the data. HMC samples from this distribution to estimate the parameters and their uncertainties.

#### Markov Chain Sampling Methods

To generate samples from a distribution \( \pi(\mathbf{q}) \) in a high-dimensional space, methods like Markov Chain Monte Carlo (MCMC) are used. MCMC navigates parameter space and selects points based on an acceptance-probability criterion, focusing on the "typical set" where the target distribution is significant.

In high dimensions, traditional MCMC methods like Gaussian Metropolis-Hastings struggle with efficiency. This is where HMC steps in, using a physics-inspired approach by treating parameters as particles governed by Hamilton's equations.

#### HMC in Action

HMC doubles the parameter space by introducing momentum \( \mathbf{p} \), creating a joint probability distribution \( \pi(\mathbf{q}, \mathbf{p}) \). Using Hamiltonian mechanics, the system evolves with symplectic integration, simulating the motion of particles in a potential energy landscape. This approach generates proposals with a high acceptance rate, making HMC highly efficient in high dimensions.

### Methods

Our implementation used JAX, a GPU-accelerated version of NumPy with automatic differentiation and parallelization capabilities. The probabilistic models were set up with Numpyro, and the log-posterior distributions were sampled using our custom HMC kernel.

| ![Parallel Architecture]({{site.baseurl}}/assets/images/hmc/flow.png) | 
|:--:| 
| *Parallel architecture using multiple GPUs.* |

### Results

We successfully fitted probabilistic models on multiple GPUs. Our implementation scales well, as shown in the following figures.

| ![Speedup with Number of GPUs]({{site.baseurl}}/assets/images/hmc/speedup.png) |
|:--:| 
| *Speedup vs. number of V100 GPUs for various numbers of particles.* |


### Discussion/Conclusions

Our progress so far is promising, and we plan to explore larger models and investigate how tuning physical parameters affects performance. One promising direction is simulated annealing, where the temperature is gradually lowered to better explore the potential landscape.

Profiling indicates significant host-device communication, which, along with the small models used, explains the limited scalability. Optimizing communication and increasing model size should lead to further performance improvements.

| ![Speedup vs Number of Particles]({{ site.baseurl }}/assets/images/hmc/scaling_2.png) |
|:--:| 
| *Speedup vs. Number of Particles on one V100 GPU.* |

### References

- [Stan Reference Manual](https://mc-stan.org/docs/reference-manual/index.html), Version 2.30, Chapter 15.
- [Numpyro Documentation](https://num.pyro.ai/en/stable/mcmc.html).
- Betancourt, M. (2017). A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434.
- Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1), 1593-1623.
